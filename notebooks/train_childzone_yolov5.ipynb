{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/changsin/ClassifyImages/blob/main/notebooks/train_dashboard_top15.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "l3IR2E-4bPaO"
      },
      "source": [
        "# ChildZone Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmr3ahqma9uk"
      },
      "source": [
        "# Setup\n",
        "Install requirements and prepare the dataset for training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPDC6ckKkr1l",
        "outputId": "dd29a929-71ed-4e77-e79f-fa718a3deafd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7fNDvNWOWvpl"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output \n",
        "\n",
        "!pip install pafy\n",
        "!pip install -q youtube-dl\n",
        "\n",
        "!pip install yolov5\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Sp8S964yARl",
        "outputId": "d72a589e-2b3e-47eb-9372-ccebe8370387"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setup complete. Using torch 1.9.0+cu111 (Tesla P100-PCIE-16GB)\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt  # install dependencies\n",
        "\n",
        "import torch\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "\n",
        "clear_output()\n",
        "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccpQvXLlatKP"
      },
      "source": [
        "Download pretrained yolov5 model\n",
        "Choose one of the pretrained models from https://github.com/ultralytics/yolov5#inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5pTcJ1fyG5T",
        "outputId": "69a60b5d-abfc-49cc-8368-862baa7578b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2021-10-16 07:51:43--  https://github.com/ultralytics/yolov5/releases/download/v5.0/yolov5s.pt\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-releases.githubusercontent.com/264818686/56dd3480-9af3-11eb-9c92-3ecd167961dc?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20211016%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20211016T075143Z&X-Amz-Expires=300&X-Amz-Signature=d0896974b764f558700d3af775b495cdb6afb8b78e64ba99cc91ba7327ac3102&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=264818686&response-content-disposition=attachment%3B%20filename%3Dyolov5s.pt&response-content-type=application%2Foctet-stream [following]\n",
            "--2021-10-16 07:51:43--  https://github-releases.githubusercontent.com/264818686/56dd3480-9af3-11eb-9c92-3ecd167961dc?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20211016%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20211016T075143Z&X-Amz-Expires=300&X-Amz-Signature=d0896974b764f558700d3af775b495cdb6afb8b78e64ba99cc91ba7327ac3102&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=264818686&response-content-disposition=attachment%3B%20filename%3Dyolov5s.pt&response-content-type=application%2Foctet-stream\n",
            "Resolving github-releases.githubusercontent.com (github-releases.githubusercontent.com)... 185.199.108.154, 185.199.111.154, 185.199.109.154, ...\n",
            "Connecting to github-releases.githubusercontent.com (github-releases.githubusercontent.com)|185.199.108.154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14795158 (14M) [application/octet-stream]\n",
            "Saving to: ‘yolov5s.pt’\n",
            "\n",
            "yolov5s.pt          100%[===================>]  14.11M  76.4MB/s    in 0.2s    \n",
            "\n",
            "2021-10-16 07:51:43 (76.4 MB/s) - ‘yolov5s.pt’ saved [14795158/14795158]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/ultralytics/yolov5/releases/download/v5.0/yolov5s.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/aidev/data/AI-Hub/ChildZoneCCTV/split\n",
            "/home/aidev/data/AI-Hub/ChildZoneCCTV/split\n",
            "total 20\n",
            "drwxrwxr-x 5 aidev aidev 4096  2월  3 12:51 .\n",
            "drwxrwxr-x 3 aidev aidev 4096  2월  3 12:07 ..\n",
            "drwxrwxr-x 9 aidev aidev 4096  2월  3 12:11 test\n",
            "drwxrwxr-x 9 aidev aidev 4096  2월  3 12:45 train\n",
            "drwxrwxr-x 9 aidev aidev 4096  2월  3 12:55 val\n"
          ]
        }
      ],
      "source": [
        "%cd /home/aidev/data/AI-Hub/ChildZoneCCTV/split\n",
        "!pwd\n",
        "!ls -al"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1SYBEUwDcFwq"
      },
      "outputs": [],
      "source": [
        "DATA_ROOT = \"/home/aidev/data/AI-Hub/ChildZoneCCTV/split/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qdNGi08turU"
      },
      "source": [
        "## Copy files (One time)\n",
        "\n",
        "To make val and test folders flat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JS8FNRNBm-z2"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "\n",
        "def glob_files(folder, file_type='*'):\n",
        "    search_string = os.path.join(folder, file_type)\n",
        "    files = glob.glob(search_string)\n",
        "\n",
        "    print('Searching ', search_string)\n",
        "    paths = []\n",
        "    for f in files:\n",
        "      if os.path.isdir(f):\n",
        "        sub_paths = glob_files(f + '/')\n",
        "        paths += sub_paths\n",
        "      else:\n",
        "        paths.append(f)\n",
        "\n",
        "    # We sort the images in alphabetical order to match them\n",
        "    #  to the annotation files\n",
        "    paths.sort()\n",
        "\n",
        "    return paths\n",
        "\n",
        "\n",
        "def glob_folders(folder, file_type='*'):\n",
        "    search_string = os.path.join(folder, file_type)\n",
        "    files = glob.glob(search_string)\n",
        "\n",
        "    print('Searching ', search_string)\n",
        "    paths = []\n",
        "    for f in files:\n",
        "      if os.path.isdir(f):\n",
        "        paths.append(f)\n",
        "\n",
        "    # We sort the images in alphabetical order to match them\n",
        "    #  to the annotation files\n",
        "    paths.sort()\n",
        "\n",
        "    return paths\n",
        "\n",
        "def split_val_files(parent_folder, folder_from, folder_to):\n",
        "  folder_to = os.path.join(parent_folder, folder_to)\n",
        "  if not os.path.exists(folder_to):\n",
        "    print(\"Creating folder to \", folder_to)\n",
        "    os.mkdir(folder_to)\n",
        "\n",
        "  sub_folders = glob_folders(folder_from)\n",
        "  copied_count = 0\n",
        "\n",
        "  for sub_id, sub_folder in enumerate(sub_folders):\n",
        "    files = glob_files(sub_folder)\n",
        "  \n",
        "    # end_id = int(len(files) * 0.2)\n",
        "    end_id = len(files)\n",
        "    print(\"Copying {} files\".format(end_id))\n",
        "\n",
        "    sub_folder_to = os.path.join(folder_to, \"{}_{}\"\n",
        "      .format(os.path.basename(folder_to), sub_id))\n",
        "    if not os.path.exists(sub_folder_to):\n",
        "      print(\"Creating folder to \", sub_folder_to)\n",
        "      os.mkdir(sub_folder_to)\n",
        "\n",
        "    for id in range(end_id):\n",
        "      file_from = files[id]\n",
        "      file_to = os.path.join(sub_folder_to, os.path.basename(file_from))\n",
        "\n",
        "      if os.path.exists(file_to):\n",
        "        print(\"ERROR: target {} already exists\".format(file_to))\n",
        "        print(\"Skipping\")\n",
        "        continue\n",
        "        # exit(-1)\n",
        "\n",
        "      else:\n",
        "        print(file_from, file_to)\n",
        "        shutil.copy(file_from, file_to)\n",
        "    copied_count += end_id\n",
        "\n",
        "  print(\"Copied \", copied_count)\n",
        "\n",
        "\n",
        "def copy_data_files(folder_from, folder_to):\n",
        "  sub_folders = glob_folders(folder_from)\n",
        "  copied_count = 0\n",
        "\n",
        "  for sub_folder in sub_folders:\n",
        "    files = glob_files(sub_folder)\n",
        "\n",
        "    for file_from in files:\n",
        "      if os.path.exists(file_from):\n",
        "          file_to = os.path.join(folder_to, os.path.basename(file_from))\n",
        "\n",
        "          if os.path.exists(file_to):\n",
        "            print(\"ERROR: target {} already exists\".format(file_to))\n",
        "            print(\"Skipping\")\n",
        "            continue\n",
        "            # exit(-1)\n",
        "\n",
        "          shutil.copy(file_from, file_to)\n",
        "          copied_count += 1\n",
        "\n",
        "  print(\"Copied \", copied_count)\n",
        "\n",
        "split_val_files(\"/content/drive/MyDrive/data/Top15\",\n",
        "                \"/content/drive/MyDrive/data/Top15/train_a_seatbelt\",\n",
        "                # \"/content/drive/MyDrive/data/Top15/val_a_seatbelt\")\n",
        "                \"/content/drive/MyDrive/data/Top15/train_a_seatbelt1\")\n",
        "# copy_data_files(DATA_ROOT + \"test_raw\", DATA_ROOT + \"test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wD1pTl7FuAVc"
      },
      "source": [
        "# Train Dashboard Labels Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ao-CwRDAt5Lt"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "import subprocess\n",
        "\n",
        "def create_yaml(yaml_from, yaml_to, to_set):\n",
        "  with open(yaml_from) as fr:\n",
        "      train_config = yaml.safe_load(fr)\n",
        "\n",
        "      for key, value in to_set.items():\n",
        "        print(\"Set {} to {}\".format(key, value))\n",
        "        train_config[key] = value\n",
        "\n",
        "      with open(yaml_to, 'w') as fw:\n",
        "        fw.write(str(train_config))\n",
        "\n",
        "def launch_process(command):\n",
        "  print(command)\n",
        "  process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE)\n",
        "  process.wait()\n",
        "  for line in process.stdout:\n",
        "      print(str(line))\n",
        "\n",
        "  print(process.stderr)\n",
        "\n",
        "  return process.stdout, process.returncode\n",
        "\n",
        "def to_file(file_to, data):\n",
        "  with open(file_to, 'w') as f:\n",
        "    f.write(str(data))\n",
        "\n",
        "def train_yolo(train_data_path, val_data_path, batch_size=10, epochs=100, weights_path=None):\n",
        "  data_yaml = DATA_ROOT + \"train_data.yaml\"\n",
        "  to_set = dict({\"train\": train_data_path, \"val\": val_data_path})\n",
        "  create_yaml(\"/home/aidev/workspace/ClassifyImages/data/configs/yolov5_child_zone.yaml\", data_yaml, to_set)\n",
        "\n",
        "  cfg_yaml = DATA_ROOT + \"train_cfg.yaml\"\n",
        "  create_yaml(\"/home/aidev/workspace/ClassifyImages/data/configs/train_cfg_child_zone.yaml\", cfg_yaml, dict({\"nc\": 10}))\n",
        "\n",
        "  if weights_path is None:\n",
        "    weights_path = \"yolov5s.pt\"\n",
        "\n",
        "  !python3 train.py --img 640 --batch $batch_size --epochs $epochs --data $data_yaml --cfg $cfg_yaml --weights $weights_path --cache\n",
        "\n",
        "# !rm -rf runs/train\n",
        "# train_yolo(DATA_ROOT+\"/train\", DATA_ROOT+\"/val\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94T2yMxwGen3",
        "outputId": "70618e34-b203-4645-d707-73c5ad22d905"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_top15_a_seatbelt_0\n",
            "train_top15_a_seatbelt_0.xml\n",
            "train_top15_a_seatbelt_1\n",
            "train_top15_a_seatbelt_1.xml\n",
            "train_top15_a_seatbelt_2\n",
            "train_top15_a_seatbelt_2.xml\n",
            "train_top15_a_seatbelt_3\n",
            "train_top15_a_seatbelt_3.xml\n",
            "train_top15_a_seatbelt_4\n",
            "train_top15_a_seatbelt_4.xml\n"
          ]
        }
      ],
      "source": [
        "!ls -Q $train_folder | head -10 | xargs -I {} echo {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/aidev/workspace/yolov5\n",
            "/home/aidev/workspace/yolov5\n",
            "total 14840\n",
            "drwxrwxr-x 12 aidev aidev     4096  2월  3 17:27 .\n",
            "drwxrwxr-x  4 aidev aidev     4096  2월  3 12:27 ..\n",
            "-rw-rw-r--  1 aidev aidev     7830  2월  3 12:27 benchmarks.py\n",
            "-rw-rw-r--  1 aidev aidev      392  2월  3 12:27 CITATION.cff\n",
            "drwxrwxr-x  2 aidev aidev     4096  2월  3 12:27 classify\n",
            "-rw-rw-r--  1 aidev aidev     4994  2월  3 12:27 CONTRIBUTING.md\n",
            "drwxrwxr-x  5 aidev aidev     4096  2월  3 12:27 data\n",
            "-rw-rw-r--  1 aidev aidev    14292  2월  3 12:27 detect.py\n",
            "-rw-rw-r--  1 aidev aidev     3701  2월  3 12:27 .dockerignore\n",
            "-rw-rw-r--  1 aidev aidev    31787  2월  3 12:27 export.py\n",
            "drwxrwxr-x  8 aidev aidev     4096  2월  3 12:27 .git\n",
            "-rw-rw-r--  1 aidev aidev       75  2월  3 12:27 .gitattributes\n",
            "drwxrwxr-x  4 aidev aidev     4096  2월  3 12:27 .github\n",
            "-rwxrwxr-x  1 aidev aidev     3998  2월  3 12:27 .gitignore\n",
            "-rw-rw-r--  1 aidev aidev     7711  2월  3 12:27 hubconf.py\n",
            "-rw-rw-r--  1 aidev aidev    35127  2월  3 12:27 LICENSE\n",
            "drwxrwxr-x  5 aidev aidev     4096  2월  3 16:49 models\n",
            "-rw-rw-r--  1 aidev aidev     1557  2월  3 12:27 .pre-commit-config.yaml\n",
            "drwxrwxr-x  2 aidev aidev     4096  2월  3 16:49 __pycache__\n",
            "-rw-rw-r--  1 aidev aidev    39531  2월  3 12:27 README.md\n",
            "-rw-rw-r--  1 aidev aidev    40113  2월  3 12:27 README.zh-CN.md\n",
            "-rw-rw-r--  1 aidev aidev     1539  2월  3 12:27 requirements.txt\n",
            "drwxrwxr-x  3 aidev aidev     4096  2월  3 16:49 runs\n",
            "drwxrwxr-x  2 aidev aidev     4096  2월  3 12:27 segment\n",
            "-rw-rw-r--  1 aidev aidev     1727  2월  3 12:27 setup.cfg\n",
            "-rw-rw-r--  1 aidev aidev    33589  2월  3 12:27 train.py\n",
            "-rw-rw-r--  1 aidev aidev    54257  2월  3 12:27 tutorial.ipynb\n",
            "drwxrwxr-x  9 aidev aidev     4096  2월  3 16:49 utils\n",
            "-rw-rw-r--  1 aidev aidev    20390  2월  3 12:27 val.py\n",
            "drwxrwxr-x  4 aidev aidev     4096  2월  4 14:45 wandb\n",
            "-rw-------  1 aidev aidev 14808437  2월  3 16:49 yolov5s.pt\n"
          ]
        }
      ],
      "source": [
        "%cd /home/aidev/workspace/yolov5\n",
        "!pwd\n",
        "!ls -al"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install Weights & Biases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: wandb in /home/aidev/.local/lib/python3.10/site-packages (0.12.17)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /home/aidev/.local/lib/python3.10/site-packages (from wandb) (3.1.30)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /home/aidev/.local/lib/python3.10/site-packages (from wandb) (5.9.4)\n",
            "Requirement already satisfied: setproctitle in /home/aidev/.local/lib/python3.10/site-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from wandb) (59.6.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/lib/python3/dist-packages (from wandb) (2.25.1)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /home/aidev/.local/lib/python3.10/site-packages (from wandb) (1.0.11)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /home/aidev/.local/lib/python3.10/site-packages (from wandb) (8.0.4)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/aidev/.local/lib/python3.10/site-packages (from wandb) (1.14.0)\n",
            "Requirement already satisfied: pathtools in /home/aidev/.local/lib/python3.10/site-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/aidev/.local/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /home/aidev/.local/lib/python3.10/site-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: PyYAML in /usr/lib/python3/dist-packages (from wandb) (5.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /home/aidev/.local/lib/python3.10/site-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/lib/python3/dist-packages (from wandb) (1.16.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /home/aidev/.local/lib/python3.10/site-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/aidev/.local/lib/python3.10/site-packages (from GitPython>=1.0.0->wandb) (4.0.10)\n",
            "Requirement already satisfied: urllib3>=1.26.11 in /home/aidev/.local/lib/python3.10/site-packages (from sentry-sdk>=1.0.0->wandb) (1.26.14)\n",
            "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from sentry-sdk>=1.0.0->wandb) (2020.6.20)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /home/aidev/.local/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Really Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VErMf-CIlVlG",
        "outputId": "cd5ca339-8ddb-4e4d-fd21-4f51db7ff3c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Set train to /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train\n",
            "Set val to /home/aidev/data/AI-Hub/ChildZoneCCTV/split/val\n",
            "Set nc to 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchangsin\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=/home/aidev/data/AI-Hub/ChildZoneCCTV/split/train_cfg.yaml, data=/home/aidev/data/AI-Hub/ChildZoneCCTV/split/train_data.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=100, batch_size=100, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
            "YOLOv5 🚀 v7.0-72-g064365d8 Python-3.10.6 torch-1.13.1+cu117 CUDA:0 (NVIDIA GeForce RTX 3080 Ti, 12037MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "2023-02-04 15:07:31.226414: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-04 15:07:31.649981: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/aidev/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
            "2023-02-04 15:07:31.650025: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/aidev/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
            "2023-02-04 15:07:31.650031: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-02-04 15:07:32.815133: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-04 15:07:32.888953: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/aidev/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
            "2023-02-04 15:07:32.888976: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "2023-02-04 15:07:33.194823: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/aidev/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
            "2023-02-04 15:07:33.194864: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/aidev/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
            "2023-02-04 15:07:33.194870: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.9 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.17\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/aidev/workspace/yolov5/wandb/run-20230204_150731-1xvwdn08\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mresplendent-cake-369\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/changsin/YOLOv5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/changsin/YOLOv5/runs/1xvwdn08\u001b[0m\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Focus                     [3, 32, 3]                    \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  3    156928  models.common.C3                        [128, 128, 3]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1    656896  models.common.SPP                       [512, 512, [5, 9, 13]]        \n",
            "  9                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     40455  models.yolo.Detect                      [10, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "train_cfg summary: 225 layers, 7087815 parameters, 7087815 gradients\n",
            "\n",
            "Transferred 308/361 items from yolov5s.pt\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 59 weight(decay=0.0), 62 weight(decay=0.00078125), 62 bias\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_21_\u001b[0m\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00091.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00095.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00096.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00097.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00098.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00099.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00100.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00101.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00102.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00103.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00104.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00105.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00106.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00107.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00108.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00110.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00111.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00112.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00113.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00114.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00115.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00117.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00118.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00119.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00121.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00122.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00123.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00124.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00125.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00126.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00130.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00133.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00136.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00137.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00138.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00139.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00140.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00145.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00146.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00148.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00149.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00150.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00151.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00159.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00160.jpg: 3 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00161.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00162.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00163.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00164.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00186.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00240.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00241.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/train/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00242.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (1.1GB ram): 100%|██████████| 1680/1680 [00:13<00:00, 120.\u001b[0m\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/aidev/data/AI-Hub/ChildZoneCCTV/split/val/10_2020_11_21_15_4\u001b[0m\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/val/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00120.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/val/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00127.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/val/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00128.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/val/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00147.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/aidev/data/AI-Hub/ChildZoneCCTV/split/val/10_2020_11_25_17_40_with_dog_night_A_01/2020_11_25_17_40_with_dog_night_A_01_00152.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.1GB ram): 100%|██████████| 210/210 [00:02<00:00, 98.32it/\u001b[0m\n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m3.13 anchors/target, 0.947 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mWARNING ⚠️ Extremely small objects found: 2045 of 32848 labels are <3 pixels in size\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 9 anchors on 32848 points...\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7490: 100%|████\u001b[0m\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.25: 1.0000 best possible recall, 4.33 anchors past thr\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=9, img_size=640, metric_all=0.286/0.749-mean/best, past_thr=0.473-mean: 6,5, 11,9, 6,20, 26,9, 12,29, 34,27, 113,60, 117,164, 256,234\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n",
            "Plotting labels to runs/train/exp6/labels.jpg... \n",
            "Image sizes 640 train, 640 val\n",
            "Using 8 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp6\u001b[0m\n",
            "Starting training for 100 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "  0%|          | 0/17 [00:00<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/aidev/workspace/yolov5/train.py\", line 634, in <module>\n",
            "    main(opt)\n",
            "  File \"/home/aidev/workspace/yolov5/train.py\", line 528, in main\n",
            "    train(opt.hyp, opt, device, callbacks)\n",
            "  File \"/home/aidev/workspace/yolov5/train.py\", line 309, in train\n",
            "    pred = model(imgs)  # forward\n",
            "  File \"/home/aidev/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/home/aidev/workspace/yolov5/models/yolo.py\", line 209, in forward\n",
            "    return self._forward_once(x, profile, visualize)  # single-scale inference, train\n",
            "  File \"/home/aidev/workspace/yolov5/models/yolo.py\", line 121, in _forward_once\n",
            "    x = m(x)  # run\n",
            "  File \"/home/aidev/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/home/aidev/workspace/yolov5/models/common.py\", line 168, in forward\n",
            "    return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), 1))\n",
            "  File \"/home/aidev/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/home/aidev/.local/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n",
            "    input = module(input)\n",
            "  File \"/home/aidev/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/home/aidev/workspace/yolov5/models/common.py\", line 121, in forward\n",
            "    return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\n",
            "  File \"/home/aidev/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/home/aidev/workspace/yolov5/models/common.py\", line 57, in forward\n",
            "    return self.act(self.bn(self.conv(x)))\n",
            "  File \"/home/aidev/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/home/aidev/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 463, in forward\n",
            "    return self._conv_forward(input, self.weight, self.bias)\n",
            "  File \"/home/aidev/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 459, in _conv_forward\n",
            "    return F.conv2d(input, weight, bias, self.stride,\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 11.75 GiB total capacity; 9.91 GiB already allocated; 40.94 MiB free; 9.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/aidev/workspace/yolov5/train.py\", line 634, in <module>\n",
            "    main(opt)\n",
            "  File \"/home/aidev/workspace/yolov5/train.py\", line 528, in main\n",
            "    train(opt.hyp, opt, device, callbacks)\n",
            "  File \"/home/aidev/workspace/yolov5/train.py\", line 309, in train\n",
            "    pred = model(imgs)  # forward\n",
            "  File \"/home/aidev/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/home/aidev/workspace/yolov5/models/yolo.py\", line 209, in forward\n",
            "    return self._forward_once(x, profile, visualize)  # single-scale inference, train\n",
            "  File \"/home/aidev/workspace/yolov5/models/yolo.py\", line 121, in _forward_once\n",
            "    x = m(x)  # run\n",
            "  File \"/home/aidev/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/home/aidev/workspace/yolov5/models/common.py\", line 168, in forward\n",
            "    return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), 1))\n",
            "  File \"/home/aidev/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/home/aidev/.local/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n",
            "    input = module(input)\n",
            "  File \"/home/aidev/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/home/aidev/workspace/yolov5/models/common.py\", line 121, in forward\n",
            "    return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\n",
            "  File \"/home/aidev/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/home/aidev/workspace/yolov5/models/common.py\", line 57, in forward\n",
            "    return self.act(self.bn(self.conv(x)))\n",
            "  File \"/home/aidev/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/home/aidev/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 463, in forward\n",
            "    return self._conv_forward(input, self.weight, self.bias)\n",
            "  File \"/home/aidev/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 459, in _conv_forward\n",
            "    return F.conv2d(input, weight, bias, self.stride,\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 11.75 GiB total capacity; 9.91 GiB already allocated; 40.94 MiB free; 9.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 1).\u001b[0m Press Control-C to abort syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mresplendent-cake-369\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/changsin/YOLOv5/runs/1xvwdn08\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230204_150731-1xvwdn08/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "train_folder = DATA_ROOT + \"train\"\n",
        "val_folder = DATA_ROOT + \"val\"\n",
        "\n",
        "train_yolo(train_folder,\n",
        "           val_folder,\n",
        "           batch_size=100,\n",
        "           epochs=100,\n",
        "           weights_path=\"yolov5s.pt\")\n",
        "\n",
        "# !mv runs/train/exp /content/drive/MyDrive/data/Top15/runs/train/train15_0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ki3fla1eLG3g"
      },
      "source": [
        "# Validate with Test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLP0zOJQYERZ",
        "outputId": "acdc17c4-263c-4aaa-c286-be2da0885aa1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Set val to /content/drive/MyDrive/data/Top15/test_top15/top15_0\n",
            "\u001b[34m\u001b[1mval: \u001b[0mdata=/content/drive/MyDrive/data/Top15/validate.yaml, weights=['/content/drive/MyDrive/data/Top15/runs/train/train15_0/weights/best.pt'], batch_size=32, imgsz=640, conf_thres=0.5, iou_thres=0.6, task=val, device=, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=False\n",
            "YOLOv5 🚀 v6.0-16-g6d9b99f torch 1.9.0+cu111 CUDA:0 (Tesla P100-PCIE-16GB, 16280.875MB)\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 213 layers, 7085641 parameters, 0 gradients, 16.0 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/drive/MyDrive/data/Top15/test_top15/top15_0' images and labels...1000 found, 0 missing, 0 empty, 0 corrupted: 100% 1000/1000 [01:58<00:00,  8.43it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/drive/MyDrive/data/Top15/test_top15/top15_0.cache\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 32/32 [00:17<00:00,  1.81it/s]\n",
            "                 all       1000       4113       0.35     0.0733      0.208      0.152\n",
            "         alert@Brake       1000        340      0.971      0.194      0.583      0.452\n",
            "       alert@Coolant       1000        229          0          0          0          0\n",
            "      alert@Distance       1000        162      0.909     0.0617      0.487      0.368\n",
            "       alert@Parking       1000        468          1     0.0278      0.514      0.378\n",
            "     alert@Retaining       1000        140          0          0          0          0\n",
            "      alert@Seatbelt       1000        443      0.662      0.709      0.629      0.398\n",
            "      alert@Steering       1000        159          0          0          0          0\n",
            "         warning@ABS       1000        227          0          0          0          0\n",
            "       warning@Brake       1000        318      0.714     0.0472      0.379       0.29\n",
            "      warning@Engine       1000        469          0          0          0          0\n",
            "        warning@Fuel       1000        179          0          0          0          0\n",
            "     warning@Parking       1000        291          0          0          0          0\n",
            "warning@StabilityOff       1000        216          1     0.0602       0.53      0.394\n",
            " warning@StabilityOn       1000        234          0          0          0          0\n",
            "        warning@Tire       1000        238          0          0          0          0\n",
            "Speed: 0.1ms pre-process, 3.3ms inference, 0.8ms NMS per image at shape (32, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/val/exp3\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "def val_yolo(val_data_path, conf=0.5, weights_path=None):\n",
        "  data_yaml = DATA_ROOT + \"validate.yaml\"\n",
        "  create_yaml(DATA_ROOT + \"validate_temp.yaml\", data_yaml, dict({\"val\": val_data_path}))\n",
        "\n",
        "  if weights_path is None:\n",
        "    weights_path = \"yolov5s.pt\"\n",
        "\n",
        "  !python val.py --weights $weights_path --img 640 --conf $conf --data $data_yaml\n",
        "\n",
        "val_yolo(\"/content/drive/MyDrive/data/Top15/test_top15/top15_0\", conf=0.5, weights_path=\"/content/drive/MyDrive/data/Top15/runs/train/train15_0/weights/best.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "2iKO_l6avgM1"
      },
      "outputs": [],
      "source": [
        "val_yolo(\"/content/drive/MyDrive/data/Top15/test_top15/top15_0\", conf=0.5, weights_path=\"/content/drive/MyDrive/data/Top15/runs/train/train15_0/weights/best.pt\")\n",
        "!mv runs/val/exp /content/drive/MyDrive/data/Top15/runs/test/test15_0"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyPFeyejc/tGVDijwq+gQwMP",
      "include_colab_link": true,
      "machine_shape": "hm",
      "name": "train_dashboard_top15.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
