{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dedupe_images.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPARSFxa/XtM4vps9gV0dqZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/changsin/ClassifyImages/blob/main/notebooks/dedupe_images.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "delgJvny089O"
      },
      "source": [
        "# Scalable solutions to detect duplicate images\n",
        "\n",
        "Identical images are easier to detect if identity means pixel-wise idenity. We can use image hash, for instance, to encode each image and quickly compare hash values of two images. However, if we want to compare how similar two images are or whether two images \"very similar\" to the point where human beings cannot tell the difference, then the problem is much harder. The latter is the working definition of \"duplication\" and I want to show how you can detect duplicate images.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dyb0B4N9HFof"
      },
      "source": [
        "\n",
        "# Problems\n",
        "The first question to ask is why you want to detect duplicate images. There could be many reasons, but let me list a few reasons for machine learning purposes.\n",
        "\n",
        "**1. Bias:** Having duplicate data means that the model will be trained more on that type of data and thus will be biased. One or more duplicate data might be Okay but if there are a lot of them, the model will fail to generalize on new data.\n",
        "\n",
        "**2. Cost:** Each piece of data needs to be labeled manually and reviewed one way or the other. Having duplicate data means adding an unnecessary cost for data processing, storage, and computation.\n",
        "\n",
        "**3. Noise:** Duplicate data can lead to subtle noise in the training set too if the labeling is not done consistently or correctly.\n",
        "\n",
        "For theses reasons, we want to remove as much duplicate data as possible. However, detecting and removing duplicates manually is not scalable and thus the need for a better approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9s-pvRygKo3a"
      },
      "source": [
        "# Other Approaches\n",
        "There are a few known solutions to detect duplicates.\n",
        "\n",
        "## 1. Image hash\n",
        "Encoding each image as a hash value and then comparing hash values is a quick and easy method to check identical images. However, as you will see, this method does not work if the image is slightly altered (e.g., the image pixels are moved one pixel left). Besides, image hash only provides a binary answer: the same or not. What we need is a similarity measure.\n",
        "\n",
        "## 2. Traditional approach\n",
        "Then there are traditional approaches to detect images similarities like taking differences of pixel values, comparing histograms, calculating structural similarity index [SSIM](https://en.wikipedia.org/wiki/Structural_similarity), or [feature matching](https://medium.com/data-breach/introduction-to-feature-detection-and-matching-65e27179885d). The pros and cons of each approach is beyond the scope of the current article, but in general they are good for comparing two or a few images at a time but hard to use at a large scale.\n",
        " \n",
        "## 3. Deep Learning\n",
        "The third approach is to leverage Deep Learning to find duplicates: e.g., a [Siamese network](https://conferences.oreilly.com/strata/strata-eu-2018/cdn.oreillystatic.com/en/assets/1/event/267/Using%20Siamese%20CNNs%20for%20removing%20duplicate%20entries%20from%20real%20estate%20listing%20databases%20Presentation.pdf). While this approach is the most robust and can handle even rotated duplicate images, it requires the heavy lifting of training and inferencing using another neural network.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFE0u_Fs6fSX"
      },
      "source": [
        "# Requirements\n",
        "In light of the survey of other approaches, here are the requirements that we would like to have:\n",
        "\n",
        "1. Similarity meausres: Instead of a single binary decision of match or non-match, we want metrics that can tell us how similar two given images are.\n",
        "2. Scalable: The solution should work on two images, multiple images, and a large dataset of images.\n",
        "3. Light-weight: Removing duplicates is for pre-processing training data for Deep Learning so we want the solution to be as light-weight as possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGbQlorzE5Q2"
      },
      "source": [
        "# Solution\n",
        "\n",
        "The solution I propose is a modified clustering algorithm. The steps are:\n",
        "\n",
        "1. Cluster an initial dataset based on feature maps\n",
        "2. Save the centroid values.\n",
        "3. Load the centroids when processing additional image data\n",
        "4. For each image, compare its feature map with centroids and find the cluster it belongs to.\n",
        "5. Compare similarity measures with other images within the cluster.\n",
        "\n",
        "To make it scalable, each cluster should be kept at a few hundred images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCPMjZDGHIwX"
      },
      "source": [
        "# Implementation\n",
        "\n",
        "For the actual implementation, first we will pass images through a CNN (Convolutional Neural Network) to extract the feature maps and then use them to find clusters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moh9YKbo3BjN"
      },
      "source": [
        "## Image hash demo\n",
        "\n",
        "The image hash works well to find identical images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4o812cZX3tKd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0221ab5c-e03a-4be7-fa03-7b34727e4f5b"
      },
      "source": [
        "!git clone https://github.com/changsin/ClassifyImages.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ClassifyImages'...\n",
            "remote: Enumerating objects: 1078, done.\u001b[K\n",
            "remote: Counting objects: 100% (1078/1078), done.\u001b[K\n",
            "remote: Compressing objects: 100% (942/942), done.\u001b[K\n",
            "remote: Total 1078 (delta 158), reused 911 (delta 46), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (1078/1078), 241.64 MiB | 16.84 MiB/s, done.\n",
            "Resolving deltas: 100% (158/158), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KGbm-O4msTgB",
        "outputId": "861de561-5e93-4f7f-9380-08b634faac36"
      },
      "source": [
        "import hashlib\n",
        "\n",
        "# https://stackoverflow.com/questions/26000198/what-does-colon-equal-in-python-mean\n",
        "def get_hash(img_path):\n",
        "  # This function will return the `md5` checksum for any input image.\n",
        "  with open(img_path, \"rb\") as f:\n",
        "    img_hash = hashlib.md5()\n",
        "    chunk = f.read()\n",
        "    while chunk:\n",
        "      img_hash.update(chunk)\n",
        "      chunk = f.read()\n",
        "  return img_hash.hexdigest()\n",
        "\n",
        "sample_image_path = '/content/ClassifyImages/data/test/david-brooke-martin-t_ZdxJsE8iM-unsplash.jpg'\n",
        "get_hash(sample_image_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'aeba119dcd4359b1d24c187fce013941'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5qhAxYVtOUK",
        "outputId": "52733b2b-7375-442b-ca24-6abe5293e69b"
      },
      "source": [
        "import cv2\n",
        "\n",
        "x = cv2.imread(sample_image_path)\n",
        "\n",
        "x.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(800, 1200, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7dgiWXC6Nrz"
      },
      "source": [
        "However, if you slightly modify the image by removing a single column of pixels, the two images look the same but the image has is completely different."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhCj0iycusiY",
        "outputId": "b364f2fe-9e42-4398-8f94-560b28bcd5e9"
      },
      "source": [
        "cv2.imwrite('test.jpg', x[1:, 1:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "aczngZx9wrSi",
        "outputId": "061712fd-f76d-4944-a74b-e348c612a973"
      },
      "source": [
        "get_hash('test.jpg')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'0d2bbd567c141e45e33b29da07d3851e'"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i549sZgjuq4S"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdGZE4vyvcCM"
      },
      "source": [
        "plt.imshow(x[1:, 1:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiOQaeU36y--"
      },
      "source": [
        "## Cluster Solution\n",
        "To handle duplicate images, we need a more robust solution. As outlined above, the first step is to create clusters out of the input images.\n",
        "\n",
        "For a demo purpose, I am using a short video clip of a river scenery. On this particular day, I was lucky enough to witness a double rainbow hung along the Han River in Seoul just when the sun was going down. The video clip will show both the double rainbow and the sunset in a single take. What I expect is that there should be two clusters. The first is about the double rainbow and the second the sunset. Let's see if the clustering algorithm can do that. \n",
        "Now let's see how "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8s_cclP58Fqo"
      },
      "source": [
        "### Sample images\n",
        "\n",
        "Let's download the demo clip and extract frame images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI3z5wmD069e"
      },
      "source": [
        "!pip install -q youtube-dl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gPvA1xA0_Iq"
      },
      "source": [
        "from IPython.display import YouTubeVideo\n",
        "\n",
        "rainbow_sunset = \"rainbow_sunset\"\n",
        "rainbow_sunset_id = 'I1wDZICq8XY'\n",
        "YouTubeVideo(rainbow_sunset_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sYeWysp1VMm",
        "outputId": "4172e024-3c7f-40ca-a085-f29f3e10d305"
      },
      "source": [
        "def download_youtube(youtube_id, save_filename):\n",
        "  !youtube-dl -f 'bestvideo[ext=mp4]' --output $save_filename\".%(ext)s\" https://www.youtube.com/watch?v=$youtube_id\n",
        "\n",
        "download_youtube(rainbow_sunset_id, rainbow_sunset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] I1wDZICq8XY: Downloading webpage\n",
            "[youtube] I1wDZICq8XY: Downloading MPD manifest\n",
            "[download] Destination: rainbow_sunset.mp4\n",
            "\u001b[K[download] 100% of 1.26MiB in 00:28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSydgmv81v04",
        "outputId": "ef55b3d3-2ae9-4349-b214-7b2ca796aeea"
      },
      "source": [
        "def to_images(youtube_id, save_folder):\n",
        "  !test -d $save_folder && rm $save_folder/*\n",
        "  !mkdir $save_folder\n",
        "  !ffmpeg -i $youtube_id\".mp4\" -filter:v fps=10 $save_folder/out%05d.jpg\n",
        "\n",
        "to_images(rainbow_sunset, rainbow_sunset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers\n",
            "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
            "  libavutil      55. 78.100 / 55. 78.100\n",
            "  libavcodec     57.107.100 / 57.107.100\n",
            "  libavformat    57. 83.100 / 57. 83.100\n",
            "  libavdevice    57. 10.100 / 57. 10.100\n",
            "  libavfilter     6.107.100 /  6.107.100\n",
            "  libavresample   3.  7.  0 /  3.  7.  0\n",
            "  libswscale      4.  8.100 /  4.  8.100\n",
            "  libswresample   2.  9.100 /  2.  9.100\n",
            "  libpostproc    54.  7.100 / 54.  7.100\n",
            "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'rainbow_sunset.mp4':\n",
            "  Metadata:\n",
            "    major_brand     : dash\n",
            "    minor_version   : 0\n",
            "    compatible_brands: iso6avc1mp41\n",
            "    creation_time   : 2021-09-12T14:49:55.000000Z\n",
            "  Duration: 00:00:07.34, start: 0.000000, bitrate: 1434 kb/s\n",
            "    Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p(tv, bt709, progressive), 1280x720 [SAR 1:1 DAR 16:9], 1014 kb/s, 29.97 fps, 29.97 tbr, 30k tbn, 59.94 tbc (default)\n",
            "    Metadata:\n",
            "      creation_time   : 2021-09-12T14:49:55.000000Z\n",
            "      handler_name    : ISO Media file produced by Google Inc.\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (h264 (native) -> mjpeg (native))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;34m[swscaler @ 0x55d90f726000] \u001b[0m\u001b[0;33mdeprecated pixel format used, make sure you did set range correctly\n",
            "\u001b[0mOutput #0, image2, to 'rainbow_sunset/out%05d.jpg':\n",
            "  Metadata:\n",
            "    major_brand     : dash\n",
            "    minor_version   : 0\n",
            "    compatible_brands: iso6avc1mp41\n",
            "    encoder         : Lavf57.83.100\n",
            "    Stream #0:0(und): Video: mjpeg, yuvj420p(pc), 1280x720 [SAR 1:1 DAR 16:9], q=2-31, 200 kb/s, 10 fps, 10 tbn, 10 tbc (default)\n",
            "    Metadata:\n",
            "      creation_time   : 2021-09-12T14:49:55.000000Z\n",
            "      handler_name    : ISO Media file produced by Google Inc.\n",
            "      encoder         : Lavc57.107.100 mjpeg\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/200000 buffer size: 0 vbv_delay: -1\n",
            "frame=   73 fps= 66 q=24.8 Lsize=N/A time=00:00:07.30 bitrate=N/A speed=6.62x    \n",
            "video:1221kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: unknown\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZWJyePe7oXT"
      },
      "source": [
        "### Extract feature maps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6c5ZN_Pl7nkt"
      },
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.models import Model\n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn import preprocessing  # to normalise existing X\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vg7RgDSE6zBB"
      },
      "source": [
        "\"\"\"\n",
        "Methods for loading and visualizing images\n",
        "\"\"\"\n",
        "\n",
        "IMAGE_SIZE = 320\n",
        "\n",
        "def glob_files(folder, file_type='*'):\n",
        "    search_string = os.path.join(folder, file_type)\n",
        "    files = glob.glob(search_string)\n",
        "\n",
        "    # print('searching ', path)\n",
        "    paths = []\n",
        "    for f in files:\n",
        "      if os.path.isdir(f):\n",
        "        sub_paths = glob_files(f + '/')\n",
        "        paths += sub_paths\n",
        "      else:\n",
        "        paths.append(f)\n",
        "\n",
        "    # We sort the images in alphabetical order to match them\n",
        "    #  to the annotation files\n",
        "    paths.sort()\n",
        "\n",
        "    return paths\n",
        "\n",
        "def load_images(path, file_type=\"*\"):\n",
        "    files = glob_files(path, file_type)\n",
        "\n",
        "    images = []\n",
        "    for file in files:\n",
        "        # print(file)\n",
        "        image = cv2.imread(file)\n",
        "        if image is not None:\n",
        "            image = cv2.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
        "            # normalize\n",
        "            image = image / 256\n",
        "            images.append(image)\n",
        "        else:\n",
        "            print(file, ' is not an image file')\n",
        "\n",
        "    return np.array(images)\n",
        "\n",
        "def plot_images(X, idx=None, limit=20):\n",
        "  fig = plt.figure(figsize=(50,60))\n",
        "\n",
        "  # The number of images for plotting is limited to 50\n",
        "  end_id = len(X) if len(X) < limit else limit\n",
        "  if idx is None:\n",
        "    idx = range(0, end_id)\n",
        "\n",
        "  i = 0\n",
        "  for id in idx:\n",
        "    axis = fig.add_subplot(5, 5, i+1)\n",
        "    plt.axis('off')\n",
        "    image = X[id]\n",
        "    plt.imshow(image)\n",
        "    i += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvvdNswO7jv8"
      },
      "source": [
        "images = load_images(rainbow_sunset)\n",
        "images.shape\n",
        "plot_images(images, limit=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H22V_hKz9D_9"
      },
      "source": [
        "# Cluster"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FLJJH9w8R3s"
      },
      "source": [
        "def get_pca_reduced(X_features, dimensions=2):\n",
        "  X_features_flatten = X_features.reshape(X_features.shape[0], -1)\n",
        "  pca = PCA(dimensions)\n",
        "\n",
        "  X_features_pca_reduced = pca.fit_transform(X_features_flatten)\n",
        "\n",
        "  return X_features_pca_reduced, pca\n",
        "\n",
        "\n",
        "def get_clusters(X_reduced, K):\n",
        "  kmeans = KMeans(n_clusters=K, random_state=0)\n",
        "  X_clusters = kmeans.fit(X_reduced)\n",
        "\n",
        "  return X_clusters, kmeans\n",
        "\n",
        "def to_cluster_idx(cluster_labels, bins):\n",
        "    \"\"\"\n",
        "    param labels: cluster labels\n",
        "    param bins: range of K\n",
        "    returns: dictionary of cluster IDs\n",
        "    \"\"\"\n",
        "    cluster_dict = dict()\n",
        "    for cluster_id in bins:\n",
        "        cluster_dict[cluster_id] = np.where(cluster_labels == cluster_id)[0]\n",
        "    return cluster_dict\n",
        "\n",
        "def cluster_images(path, K=2):\n",
        "  X = load_images(path)\n",
        "  plot_images(X)\n",
        "  X_reduced, pca = get_pca_reduced(X, dimensions=K)\n",
        "\n",
        "  X_clusters, kmeans = get_clusters(X_reduced, K)\n",
        "\n",
        "  # get the image ids of each cluster\n",
        "  cluster_idx = to_cluster_idx(X_clusters.labels_, range(K))\n",
        "\n",
        "  # keep the cluster centers\n",
        "  print(kmeans.cluster_centers_)\n",
        "  print(cluster_idx)\n",
        "  \n",
        "  return X_reduced, kmeans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYpjN2ti9GZK"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_data_in_clusters(data, kmeans, idx=None, show_centroids=True):\n",
        "  marker_size = 7\n",
        "\n",
        "  # Plot the decision boundary. For that, we will assign a color to each\n",
        "  x_min, x_max = data[:, 0].min(), data[:, 0].max()\n",
        "  y_min, y_max = data[:, 1].min(), data[:, 1].max()\n",
        "\n",
        "  # Step size of the mesh. Decrease to increase the quality of the VQ.\n",
        "  # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "  h = float((x_max - x_min)/100)\n",
        "\n",
        "  PADDING = h * marker_size\n",
        "  x_min, x_max = x_min - PADDING, x_max + PADDING\n",
        "  y_min, y_max = y_min - PADDING, y_max + PADDING\n",
        "\n",
        "  xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "\n",
        "  # Obtain labels for each point in mesh. Use last trained model.\n",
        "  Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "  # Put the result into a color plot\n",
        "  Z = Z.reshape(xx.shape)\n",
        "\n",
        "  plt.figure(2)\n",
        "  # plt.clf()\n",
        "  plt.imshow(Z, interpolation=\"nearest\",\n",
        "              extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
        "              cmap=plt.cm.Paired, aspect=\"auto\", origin=\"lower\")\n",
        "\n",
        "  plt.plot(data[:, 0], data[:, 1], 'k.', markersize=marker_size)\n",
        "\n",
        "  if show_centroids:\n",
        "    markers = [\"o\", \"1\"]\n",
        "    # Plot the centroids as a white X\n",
        "    centroids = kmeans.cluster_centers_\n",
        "\n",
        "    for id in range(len(centroids)):\n",
        "      c = centroids[id]\n",
        "      plt.scatter(c[0], c[1], marker=markers[id], s=150, linewidths=marker_size,\n",
        "                  color=\"w\", zorder=10)\n",
        "  if idx:\n",
        "    for id in idx:\n",
        "        plt.scatter(data[id, 0], data[id, 1], marker=\"x\",\n",
        "                    s=150, linewidths=marker_size,\n",
        "                    color=\"w\", zorder=10)\n",
        "\n",
        "  plt.title(\"K-means clustering\")\n",
        "  plt.xlim(x_min, x_max)\n",
        "  plt.ylim(y_min, y_max)\n",
        "  plt.xticks(())\n",
        "  plt.yticks(())\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hzcw1xP99Lxc"
      },
      "source": [
        "X_reduced_rs, kmeans_rs = cluster_images(rainbow_sunset)\n",
        "plot_data_in_clusters(X_reduced_rs, kmeans=kmeans_rs, idx=[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUFrAmy_992F"
      },
      "source": [
        "plot_images(images, idx=[55, 56, 57])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tdHwT2UlVBp"
      },
      "source": [
        "## Save the centroids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ReRIvdTkpGM"
      },
      "source": [
        "# A new video - migrating birds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiipNu7x-bug"
      },
      "source": [
        "migrating_birds = \"migrating_birds\"\n",
        "migrating_birds_id = '-0jhgfyzINQ'\n",
        "YouTubeVideo(migrating_birds_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJZlY20gkmjH",
        "outputId": "3e3231d8-2140-4e93-eb91-494a7857b4cf"
      },
      "source": [
        "download_youtube(migrating_birds_id, migrating_birds)\n",
        "to_images(migrating_birds, migrating_birds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] -0jhgfyzINQ: Downloading webpage\n",
            "[youtube] -0jhgfyzINQ: Downloading MPD manifest\n",
            "[download] Destination: migrating_birds.mp4\n",
            "\u001b[K[download] 100% of 2.36MiB in 00:00\n",
            "rm: cannot remove 'migrating_birds/*': No such file or directory\n",
            "mkdir: cannot create directory ‘migrating_birds’: File exists\n",
            "ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers\n",
            "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
            "  libavutil      55. 78.100 / 55. 78.100\n",
            "  libavcodec     57.107.100 / 57.107.100\n",
            "  libavformat    57. 83.100 / 57. 83.100\n",
            "  libavdevice    57. 10.100 / 57. 10.100\n",
            "  libavfilter     6.107.100 /  6.107.100\n",
            "  libavresample   3.  7.  0 /  3.  7.  0\n",
            "  libswscale      4.  8.100 /  4.  8.100\n",
            "  libswresample   2.  9.100 /  2.  9.100\n",
            "  libpostproc    54.  7.100 / 54.  7.100\n",
            "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'migrating_birds.mp4':\n",
            "  Metadata:\n",
            "    major_brand     : dash\n",
            "    minor_version   : 0\n",
            "    compatible_brands: iso6avc1mp41\n",
            "    creation_time   : 2021-09-13T00:27:59.000000Z\n",
            "  Duration: 00:00:09.80, start: 0.000000, bitrate: 2024 kb/s\n",
            "    Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p(tv, bt709, progressive), 1280x720 [SAR 1:1 DAR 16:9], 1090 kb/s, 29.61 fps, 29.61 tbr, 11842 tbn, 59.21 tbc (default)\n",
            "    Metadata:\n",
            "      creation_time   : 2021-09-13T00:27:59.000000Z\n",
            "      handler_name    : ISO Media file produced by Google Inc.\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (h264 (native) -> mjpeg (native))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;34m[swscaler @ 0x558c9857a000] \u001b[0m\u001b[0;33mdeprecated pixel format used, make sure you did set range correctly\n",
            "\u001b[0mOutput #0, image2, to 'migrating_birds/out%05d.jpg':\n",
            "  Metadata:\n",
            "    major_brand     : dash\n",
            "    minor_version   : 0\n",
            "    compatible_brands: iso6avc1mp41\n",
            "    encoder         : Lavf57.83.100\n",
            "    Stream #0:0(und): Video: mjpeg, yuvj420p(pc), 1280x720 [SAR 1:1 DAR 16:9], q=2-31, 200 kb/s, 10 fps, 10 tbn, 10 tbc (default)\n",
            "    Metadata:\n",
            "      creation_time   : 2021-09-13T00:27:59.000000Z\n",
            "      handler_name    : ISO Media file produced by Google Inc.\n",
            "      encoder         : Lavc57.107.100 mjpeg\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/200000 buffer size: 0 vbv_delay: -1\n",
            "frame=   98 fps= 58 q=24.8 Lsize=N/A time=00:00:09.80 bitrate=N/A speed=5.79x    \n",
            "video:3053kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: unknown\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDVsjpfXlBqr"
      },
      "source": [
        "def to_json(path, data):\n",
        "    \"\"\"\n",
        "    save json data to path\n",
        "    \"\"\"\n",
        "    with open(path, 'w', encoding='utf-8') as file:\n",
        "        json.dump(data, file, ensure_ascii=False, indent=4)\n",
        "\n",
        "def from_json(path):\n",
        "    \"\"\"\n",
        "    save json data to path\n",
        "    \"\"\"\n",
        "    file = open(path, 'r', encoding='utf-8')\n",
        "    return json.load(file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTb-D_S3nmmb"
      },
      "source": [
        "images_mb = load_images(migrating_birds)\n",
        "images_mb.shape\n",
        "plot_images(images_mb, limit=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abCJr6Tgl5Ws",
        "outputId": "2a495bca-881a-4db8-c072-5e345eea1fbc"
      },
      "source": [
        "kmeans_rs.cluster_centers_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 79.34138652,  -4.37693964],\n",
              "       [-41.32363881,   2.27965606]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1DO-h2gmwJk"
      },
      "source": [
        "\n",
        "\n",
        "# Calculate distances of all points\n",
        "distances = cdist(X_train_pca, X_train_pca)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}